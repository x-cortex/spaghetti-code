{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, TextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "import tempfile\n",
    "\n",
    "persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "\n",
    "raw = TextLoader(\"processed_chat.txt\").load()\n",
    "print(raw)\n",
    "\n",
    "\n",
    "# Load documents\n",
    "class Every20LinesSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        lines = text.split(\"\\n\")\n",
    "        return [\"\\n\".join(lines[i : i + 10]) for i in range(0, len(lines), 10)]\n",
    "\n",
    "\n",
    "# Split documents every 20 lines\n",
    "text_splitter = Every20LinesSplitter(chunk_overlap=0)\n",
    "docs = text_splitter.create_documents([raw[0].page_content])\n",
    "\n",
    "# Add to vectorDB\n",
    "vecstore = SKLearnVectorStore.from_documents(\n",
    "    docs[:20],\n",
    "    NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_path=persist_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecstore.persist()\n",
    "print(\"Vector store was persisted to\", persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vecstore.as_retriever(k=3)\n",
    "print(retriever.invoke(\"valo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LangChain Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"dictionary that contains information we want to propagate to, and modify in, each graph node.\"\"\"\n",
    "\n",
    "    chat_message: str  # User question\n",
    "    documents: List[str]  # List of retrieved documents from vector store or web search\n",
    "    generation: str  # LLM generation\n",
    "    web_search: str  # Binary decision to run web search\n",
    "    max_retries: int  # Max number of retries for answer generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or quick-response.\n",
    "The vectorstore contains documents related to chat related to various general topic wtih more depth and more tokens.                    \n",
    "Use the vectorstore for questions on these topics. For all else, and especially small and generic topics .\n",
    "Return JSON with single key, datasource, that is 'quick-response' or 'vectorstore' depending on the question.\n",
    "\"\"\"\n",
    "# You are a grader assessing relevance of a retrieved document to a user question.\n",
    "# If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user chat message.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "# Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}.\n",
    "# This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "# Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here are few of the user chat message \\n\\n {chat_message}\n",
    "Observe this carefully and give a binary score to indicate whether the document contains at least some information that is relevant to the question.\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = \"\"\"You are responding as a person to a chat response. Here is the context to use to answer the question:\n",
    "{context} \n",
    "Think about this context carefully and give an appropriate response to the following set of chat messages: {chat_message}\n",
    "Provide reply in a conversational manner using only the context provided. Use one sentence maximum.\n",
    "\"\"\"\n",
    "\n",
    "quick_response_prompt = \"\"\"You are responding to a person's chat message.\n",
    "Here is the chat message: {chat_message}\n",
    "Its a generic chat message. So keep the response short and concise. Dont use more than 3 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "test_web_search = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_web_search_2 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n",
    ")\n",
    "test_vector_store = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)] + [HumanMessage(content=\"wassup\")]\n",
    ")\n",
    "print(\n",
    "    json.loads(test_web_search.content),\n",
    "    json.loads(test_web_search_2.content),\n",
    "    json.loads(test_vector_store.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"Route question to web search or RAG\"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    route_question = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions)]\n",
    "        + [HumanMessage(content=state[\"chat_message\"])]\n",
    "    )\n",
    "    source = json.loads(route_question.content)[\"datasource\"]\n",
    "    if source == \"quick-response\":\n",
    "        print(\"---ROUTE QUESTION TO QUICK-RESPONSE---\")\n",
    "        return \"quick-response\"\n",
    "    elif source == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "# Retrive documents from vector store\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    chat_message = state[\"chat_message\"]\n",
    "\n",
    "    documents = retriever.invoke(chat_message)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# Grade retrieved documents from vector store\n",
    "# def grade_documents(state):\n",
    "#     \"\"\"If any document is not relevant, we will set a flag to run web search\"\"\"\n",
    "\n",
    "#     print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "#     chat_message = state[\"chat_message\"]\n",
    "#     documents = state[\"documents\"]\n",
    "\n",
    "#     # Score each doc\n",
    "#     filtered_docs = []\n",
    "#     for d in documents:\n",
    "#         doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "#             document=d.page_content, chat_message=chat_message\n",
    "#         )\n",
    "#         result = llm_json_mode.invoke(\n",
    "#             [SystemMessage(content=doc_grader_instructions)]\n",
    "#             + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "#         )\n",
    "#         grade = json.loads(result.content)[\"binary_score\"]\n",
    "#         # Document relevant\n",
    "#         if grade.lower() == \"yes\":\n",
    "#             print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "#             filtered_docs.append(d)\n",
    "#     return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# def decide_to_generate(state):\n",
    "#     \"\"\"Determines whether to generate an answer, or add web search\"\"\"\n",
    "\n",
    "#     print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "#     question = state[\"question\"]\n",
    "#     web_search = state[\"web_search\"]\n",
    "#     filtered_documents = state[\"documents\"]\n",
    "\n",
    "#     if web_search == \"Yes\":\n",
    "#         # All documents have been filtered check_relevance\n",
    "#         # We will re-generate a new query\n",
    "#         print(\n",
    "#             \"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "#         )\n",
    "#         return \"websearch\"\n",
    "#     else:\n",
    "#         # We have relevant documents, so generate answer\n",
    "#         print(\"---DECISION: GENERATE---\")\n",
    "#         return \"generate\"\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Generate answer using RAG on retrieved documents\"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    chat_message = state[\"chat_message\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    rag_prompt_formatted = rag_prompt.format(\n",
    "        context=docs_txt, chat_message=chat_message\n",
    "    )\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "\n",
    "# Web search based based on the question\n",
    "def quick_response(state):\n",
    "    \"\"\"Generate answer using quick-response\"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    chat_message = state[\"chat_message\"]\n",
    "    # documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "    # docs_txt = format_docs(documents)\n",
    "    rag_prompt_formatted = quick_response_prompt.format(chat_message=chat_message)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "\n",
    "# def grade_generation_v_documents_and_question(state):\n",
    "#     \"\"\"Determines whether the generation is grounded in the document and answers question\"\"\"\n",
    "\n",
    "#     print(\"---CHECK HALLUCINATIONS---\")\n",
    "#     question = state[\"question\"]\n",
    "#     documents = state[\"documents\"]\n",
    "#     generation = state[\"generation\"]\n",
    "#     max_retries = state.get(\"max_retries\", 3)  # Default to 3 if not provided\n",
    "\n",
    "#     hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "#         documents=format_docs(documents), generation=generation.content\n",
    "#     )\n",
    "#     result = llm_json_mode.invoke(\n",
    "#         [SystemMessage(content=hallucination_grader_instructions)]\n",
    "#         + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    "#     )\n",
    "#     grade = json.loads(result.content)[\"binary_score\"]\n",
    "\n",
    "#     # Check hallucination\n",
    "#     if grade == \"yes\":\n",
    "#         print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "#         # Check question-answering\n",
    "#         print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "#         # Test using question and generation from above\n",
    "#         answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "#             question=question, generation=generation.content\n",
    "#         )\n",
    "#         result = llm_json_mode.invoke(\n",
    "#             [SystemMessage(content=answer_grader_instructions)]\n",
    "#             + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    "#         )\n",
    "#         grade = json.loads(result.content)[\"binary_score\"]\n",
    "#         if grade == \"yes\":\n",
    "#             print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "#             return \"useful\"\n",
    "#         elif state[\"loop_step\"] <= max_retries:\n",
    "#             print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "#             return \"not useful\"\n",
    "#         else:\n",
    "#             print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "#             return \"max retries\"\n",
    "#     elif state[\"loop_step\"] <= max_retries:\n",
    "#         print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "#         return \"not supported\"\n",
    "#     else:\n",
    "#         print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "#         return \"max retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"quick-response\", quick_response)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "# workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"quick-response\": \"quick-response\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "# workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "# workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"grade_documents\",\n",
    "#     decide_to_generate,\n",
    "#     {\n",
    "#         \"websearch\": \"websearch\",\n",
    "#         \"generate\": \"generate\",\n",
    "#     },\n",
    "# )\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"generate\",\n",
    "#     grade_generation_v_documents_and_question,\n",
    "#     {\n",
    "#         \"not supported\": \"generate\",\n",
    "#         \"useful\": END,\n",
    "#         \"not useful\": \"websearch\",\n",
    "#         \"max retries\": END,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "g = workflow.compile()\n",
    "display(Image(g.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"chat_message\": \"hi\"}\n",
    "for event in g.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"chat_message\": \"do you play valorant?\",\n",
    "}\n",
    "\n",
    "events = []\n",
    "\n",
    "for event in g.stream(inputs, stream_mode=\"values\"):\n",
    "    events.append(event)\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[-1][\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
